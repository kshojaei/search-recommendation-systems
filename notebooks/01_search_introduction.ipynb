{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction to Search Systems\n",
        "\n",
        "## Learning Objectives\n",
        "By the end of this notebook, you will be able to:\n",
        "- Understand what a search system is and why it matters in e-commerce\n",
        "- Identify the key components: query processing, indexing, retrieval, ranking, presentation\n",
        "- Build and play with simple search functions over a toy product catalog\n",
        "- Evaluate search quality with precision@K, recall@K, MRR, and NDCG\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Key Components of a Search System\n",
        "\n",
        "1. Query Processing: tokenization, normalization, synonyms/typo handling\n",
        "2. Indexing: build inverted index for fast lookup\n",
        "3. Retrieval: collect candidate documents matching query terms\n",
        "4. Ranking: score candidates (e.g., TF-IDF) and order by relevance\n",
        "5. Presentation: facets, filters, suggestions, UI formatting\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample product Data (1 row):\n",
            "================================================================================\n",
            "                                                        0\n",
            "id                                                   P001\n",
            "title                                       iPhone 14 Pro\n",
            "description  Latest Apple smartphone with advanced camera\n",
            "category                                      Electronics\n",
            "price                                              999.99\n",
            "brand                                               Apple\n",
            "\n",
            "Dataset Shape: (8, 6)\n",
            "Categories: ['Electronics' 'Shoes' 'Clothing']\n",
            "Brands: ['Apple' 'Samsung' 'Nike' 'Adidas' 'Dell' 'Fashion Brand' 'Denim Co']\n",
            "Price Range: $59.99 - $2499.99\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Sample product data (tiny catalog for fast iteration)\n",
        "products_data = [\n",
        "    (\"P001\", \"iPhone 14 Pro\", \"Latest Apple smartphone with advanced camera\", \"Electronics\", 999.99, \"Apple\"),\n",
        "    (\"P002\", \"Samsung Galaxy S23\", \"Android smartphone with great camera\", \"Electronics\", 799.99, \"Samsung\"),\n",
        "    (\"P003\", \"Nike Air Max\", \"Comfortable running shoes\", \"Shoes\", 129.99, \"Nike\"),\n",
        "    (\"P004\", \"Adidas Ultraboost\", \"Premium running shoes\", \"Shoes\", 180.00, \"Adidas\"),\n",
        "    (\"P005\", \"MacBook Pro 16\", \"Professional laptop for work\", \"Electronics\", 2499.99, \"Apple\"),\n",
        "    (\"P006\", \"Dell XPS 13\", \"Ultrabook laptop\", \"Electronics\", 1199.99, \"Dell\"),\n",
        "    (\"P007\", \"Red Dress\", \"Elegant evening dress\", \"Clothing\", 89.99, \"Fashion Brand\"),\n",
        "    (\"P008\", \"Blue Jeans\", \"Classic denim jeans\", \"Clothing\", 59.99, \"Denim Co\"),\n",
        "]\n",
        "\n",
        "df_products = pd.DataFrame(products_data, columns=[\"id\",\"title\",\"description\",\"category\",\"price\",\"brand\"])\n",
        "print(\"Sample product Data (1 row):\")\n",
        "print(\"=\"*80)\n",
        "print(df_products.head(1).T)\n",
        "print(f\"\\nDataset Shape: {df_products.shape}\")\n",
        "print(f\"Categories: {df_products.category.unique()}\")\n",
        "print(f\"Brands: {df_products.brand.unique()}\")\n",
        "print(f\"Price Range: ${df_products.price.min():.2f} - ${df_products.price.max():.2f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Find all Apple products:\n",
            "            title  brand\n",
            "0   iPhone 14 Pro  Apple\n",
            "4  MacBook Pro 16  Apple\n",
            "\n",
            "Find all products under $200:\n",
            "               title   price\n",
            "2       Nike Air Max  129.99\n",
            "3  Adidas Ultraboost  180.00\n",
            "6          Red Dress   89.99\n",
            "7         Blue Jeans   59.99\n"
          ]
        }
      ],
      "source": [
        "# Quick data exploration examples\n",
        "print(\"\\nFind all Apple products:\")\n",
        "print(df_products[df_products.brand == \"Apple\"][['title','brand']])\n",
        "\n",
        "print(\"\\nFind all products under $200:\")\n",
        "print(df_products[df_products.price < 200][['title','price']])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing simple search for \"iPhone\":\n",
            "1. iPhone 14 Pro - $999.99 (Score: 2)\n"
          ]
        }
      ],
      "source": [
        "# Simple lexical search (playground)\n",
        "def simple_search(products_df: pd.DataFrame, query: str):\n",
        "    query_lower = str(query).lower()\n",
        "    results = []\n",
        "    for _, row in products_df.iterrows():\n",
        "        title_match = query_lower in str(row['title']).lower()\n",
        "        desc_match = query_lower in str(row['description']).lower()\n",
        "        if title_match or desc_match:\n",
        "            score = 2 if title_match else 1\n",
        "            if desc_match:\n",
        "                score += 1\n",
        "            results.append({'product': row.to_dict(), 'score': score})\n",
        "    results.sort(key=lambda x: x['score'], reverse=True)\n",
        "    return results\n",
        "\n",
        "print('Testing simple search for \"iPhone\":')\n",
        "for i, r in enumerate(simple_search(df_products, 'iPhone'), 1):\n",
        "    p = r['product']\n",
        "    print(f\"{i}. {p['title']} - ${p['price']} (Score: {r['score']})\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing improved search for 'running shoes':\n",
            "1. Nike Air Max - $129.99 (Score: 2, Matches: 2)\n",
            "2. Adidas Ultraboost - $180.0 (Score: 2, Matches: 2)\n"
          ]
        }
      ],
      "source": [
        "# Improved search with tokenization and multi-term matching\n",
        "def tokenize_text(text: str):\n",
        "    punctuation_chars = \".,!?;:()[]{}'\\\"-\"\n",
        "    text = text.lower()\n",
        "    for ch in punctuation_chars:\n",
        "        text = text.replace(ch, ' ')\n",
        "    return [w for w in text.split() if w]\n",
        "\n",
        "def improved_search(products_df: pd.DataFrame, query: str):\n",
        "    query_tokens = tokenize_text(query)\n",
        "    results = []\n",
        "    for _, row in products_df.iterrows():\n",
        "        searchable_tokens = tokenize_text(f\"{row['title']} {row['description']}\")\n",
        "        matches = sum(1 for t in query_tokens if t in searchable_tokens)\n",
        "        if matches > 0:\n",
        "            score = matches + (1 if any(t in row['title'].lower() for t in query_tokens) else 0)\n",
        "            results.append({'product': row.to_dict(), 'score': score, 'matches': matches})\n",
        "    results.sort(key=lambda x: x['score'], reverse=True)\n",
        "    return results\n",
        "\n",
        "print(\"Testing improved search for 'running shoes':\")\n",
        "for i, r in enumerate(improved_search(df_products, 'running shoes'), 1):\n",
        "    p = r['product']\n",
        "    print(f\"{i}. {p['title']} - ${p['price']} (Score: {r['score']}, Matches: {r['matches']})\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why these metrics vs others (quick guide)\n",
        "- Precision@K: top‑K quality; simple to interpret; ignores rank positions beyond inclusion.\n",
        "- Recall@K: coverage; useful for discovery; can overvalue less relevant items if K is large.\n",
        "- MRR: time‑to‑first‑relevant; great for navigational queries; ignores subsequent relevant items.\n",
        "- MAP: emphasizes ranking all relevant items high; binary labels; less aligned with graded relevance.\n",
        "- NDCG: graded relevance + position discounting; aligns with user behavior; normalized for cross‑query comparability.\n",
        "\n",
        "Rule of thumb:\n",
        "- Navigational → MRR/Precision@K (small K)\n",
        "- Discovery/browse → Recall@K + NDCG@K (larger K)\n",
        "- Offline robustness → NDCG@K + MAP\n",
        "- Always pair with online guardrails (CTR, Conversion, RPS, latency, zero‑results)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## How to choose the right evaluation metric (and why)\n",
        "\n",
        "- Precision@K\n",
        "  - What it measures: fraction of top-K results that are relevant\n",
        "  - When it matters most: navigational/transactional queries where users expect high-quality results at the very top (e.g., brand, SKU, \"iPhone 14 Pro\")\n",
        "  - Trade‑off: Raising precision can reduce recall if you return fewer diverse results\n",
        "\n",
        "- Recall@K\n",
        "  - What it measures: fraction of all relevant items that appear in the top K\n",
        "  - When it matters most: exploratory/browse queries, long-tail attributes, when users might scroll or filter to find more options (e.g., \"running shoes\")\n",
        "  - Trade‑off: Raising recall can lower precision by including more borderline results\n",
        "\n",
        "- MAP (Mean Average Precision)\n",
        "  - What it measures: average precision across ranks for each query, then averaged across queries\n",
        "  - Why it’s useful: rewards ranking all relevant items high (not just the first one); stable across queries with multiple relevant items\n",
        "  - When to prefer: offline relevance benchmarking with multiple relevant items per query\n",
        "\n",
        "- MRR (Mean Reciprocal Rank)\n",
        "  - What it measures: how quickly (at which rank) the first relevant item appears\n",
        "  - Why it’s useful: simple, focused on the first hit; excellent for navigational queries (e.g., user wants a specific product)\n",
        "  - When to prefer: scenarios where “first correct result fast” matters (e.g., product detail lookup)\n",
        "\n",
        "- NDCG (Normalized Discounted Cumulative Gain)\n",
        "  - What it measures: graded relevance with position discounting; rewards highly relevant results at the top\n",
        "  - Why it’s useful: captures both relevance intensity and rank position; robust to multiple relevance levels\n",
        "  - When to prefer: realistic ranking tasks with graded labels (e.g., “perfect”, “good”, “ok”); especially in e‑commerce where not all relevant items are equal\n",
        "\n",
        "- Putting it together for e‑commerce\n",
        "  - Checkout-focused searches (e.g., brand/SKU): prioritize MRR/Precision@K (K in [1,3])\n",
        "  - Browse/discovery (e.g., “black running shoes”): prioritize Recall@K and NDCG@K (K in [10,20])\n",
        "  - Overall offline evaluation: use NDCG@K and MAP for robustness; report Precision/Recall for interpretability\n",
        "  - Online A/B guardrails: CTR, Add‑to‑Cart, Conversion Rate, Revenue/Search, plus Zero‑Results Rate and latency (p95/p99)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Why NDCG (vs only Precision/Recall)\n",
        "\n",
        "1) Quick refresher:\n",
        "- Precision@K: fraction of top‑K that are relevant\n",
        "- Recall@K: fraction of all relevant items retrieved in top‑K\n",
        "\n",
        "Limitations: both treat all retrieved relevant items equally and ignore rank position; they’re binary (relevant vs not) and very sensitive to the choice of K.\n",
        "\n",
        "2) Enter NDCG: graded relevance + position discounting\n",
        "- DCG@K uses graded labels r_i (e.g., 2=highly relevant, 1=relevant, 0=irrelevant) with logarithmic discount so top ranks matter more.\n",
        "- NDCG@K = DCG@K / IDCG@K normalizes by the ideal ranking, giving a 0–1 score comparable across queries.\n",
        "\n",
        "3) Why NDCG is often better for search\n",
        "- Accounts for rank position: a relevant result at rank 1 is better than at rank 10\n",
        "- Supports graded relevance: not all relevant items are equal in e‑commerce (exact SKU match vs category match)\n",
        "- Stable across queries: normalization makes scores comparable when query difficulty or number of relevant items differ\n",
        "- Aligns with user behavior: users click top ranks more; discounts lower positions\n",
        "\n",
        "4) When to use which\n",
        "- Use MRR/Precision@K for navigational queries (find 1 best item fast)\n",
        "- Use Recall@K when coverage matters (browse/discovery, facet exploration)\n",
        "- Use NDCG@K for overall ranking quality with graded labels and position sensitivity (most e‑commerce use‑cases)\n",
        "- Use MAP when you care about ranking all relevant items high without graded labels\n",
        "\n",
        "5) Practical choices (retail search)\n",
        "- Report NDCG@K with K in {3, 10, 20} (K=3 for first row; K=10/20 for page‑level)\n",
        "- Pair with Precision@K for interpretability and Recall@K for coverage\n",
        "- Always check zero‑results rate and latency; great relevance with poor coverage/latency fails users\n",
        "\n",
        "6) Pitfalls and tips\n",
        "- Garbage in, garbage out: NDCG needs sensible graded relevance labels\n",
        "- Beware label imbalance (few positives): add multiple K’s and query buckets\n",
        "- Don’t overfit K: pick K based on UX (e.g., products above the fold)\n",
        "- Compare distributions, not just means; include confidence intervals when reporting\n",
        "\n",
        "7) Interview sound‑bites you can use\n",
        "- “Precision/Recall ignore rank; NDCG discounts lower positions, matching user scan behavior.”\n",
        "- “NDCG supports graded relevance, critical when exact match ≻ partial match.”\n",
        "- “We report NDCG@3/10 plus Precision@K and Recall@K, with business guardrails (CTR, conversion, latency).”\n",
        "- “Normalization (IDCG) makes NDCG comparable across queries with different numbers of relevant items.”\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Practice: Build intuition by tweaking the code\n",
        "\n",
        "Try these small experiments (change code above and re-run):\n",
        "\n",
        "1) Precision vs Recall\n",
        "- Change K in the `precision_at_k`/`recall_at_k` loop (e.g., K ∈ {1,2,3,5,10}). Explain why Precision usually decreases as K grows while Recall increases.\n",
        "- Modify `query_relevance` to add/remove relevant IDs. Predict and then verify impacts on Precision/Recall.\n",
        "\n",
        "2) Title boost and ranking\n",
        "- In `improved_search`, change the title boost from `+1` to `+2` or `0`. How does this affect the top-3 for queries like \"Apple\", \"running shoes\"? Why?\n",
        "- Add a small price-related boost (e.g., favor cheaper or mid-range) and explain trade-offs for business goals.\n",
        "\n",
        "3) MRR sensitivity\n",
        "- Introduce an irrelevant top-1 result for a query by altering tokenization or boosts. Observe MRR drop and explain why MRR is sensitive to the first relevant rank.\n",
        "\n",
        "4) NDCG with graded relevance\n",
        "- Replace the toy `scores` with graded labels (e.g., 2 for exact match, 1 for category match, 0 otherwise) computed from your catalog.\n",
        "- Compare NDCG@3 vs NDCG@10 for the same query. Why might they differ?\n",
        "\n",
        "5) Offline to online mapping\n",
        "- Propose an offline metric target (e.g., +0.02 NDCG@10) and map it to online guardrails (CTR, Conversion, Revenue/Search, zero-results, latency). What risks should you watch for in e‑commerce?\n",
        "\n",
        "6) Edge cases\n",
        "- Create a query with zero results. Add synonyms (e.g., \"tee\" → \"t-shirt\") to fix it; measure the change.\n",
        "- Create a highly ambiguous query (e.g., \"apple\"). How would you balance relevance with diversity? What metrics would you monitor?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## MAP (Mean Average Precision)\n",
        "\n",
        "- Per-query Average Precision (AP): average of the precision at each rank where a relevant item appears.\n",
        "\n",
        "$$\\mathrm{AP}(q) = \\frac{1}{\\lvert R_q \\rvert} \\sum_{k=1}^{K} \\mathbb{1}[rel_k] \\cdot \\mathrm{P@}k$$\n",
        "\n",
        "- Mean Average Precision over queries:\n",
        "\n",
        "$$\\mathrm{MAP} = \\frac{1}{\\lvert Q \\rvert} \\sum_{q\\in Q} \\mathrm{AP}(q)$$\n",
        "\n",
        "Why MAP: rewards ranking all relevant items high (not just the first), more stable when multiple relevant items exist per query. Prefer MAP when labels are binary and you care about full ordering of relevant items.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Per-query AP:\n",
            "  iPhone: 1.000\n",
            "  laptop: 1.000\n",
            "  shoes: 1.000\n",
            "MAP: 1.000\n"
          ]
        }
      ],
      "source": [
        "# Compute MAP over example queries using improved_search outputs\n",
        "from typing import List\n",
        "\n",
        "def average_precision(relevant: set, retrieved: List[str]) -> float:\n",
        "    if not relevant:\n",
        "        return 0.0\n",
        "    hits = 0\n",
        "    precisions = []\n",
        "    for k, rid in enumerate(retrieved, start=1):\n",
        "        if rid in relevant:\n",
        "            hits += 1\n",
        "            precisions.append(hits / k)\n",
        "    return float(np.mean(precisions)) if precisions else 0.0\n",
        "\n",
        "# Fallback local relevance mapping if not already defined above\n",
        "if 'query_relevance' not in globals():\n",
        "    query_relevance = {\n",
        "        'iPhone': {'P001'},\n",
        "        'laptop': {'P005','P006'},\n",
        "        'shoes': {'P003','P004'},\n",
        "    }\n",
        "\n",
        "queries = ['iPhone','laptop','shoes']\n",
        "APs = []\n",
        "for q in queries:\n",
        "    results = improved_search(df_products, q)\n",
        "    retrieved = [r['product']['id'] for r in results]\n",
        "    relevant = set(query_relevance[q])\n",
        "    APs.append(average_precision(relevant, retrieved))\n",
        "\n",
        "print(\"Per-query AP:\")\n",
        "for q, ap in zip(queries, APs):\n",
        "    print(f\"  {q}: {ap:.3f}\")\n",
        "print(\"MAP:\", f\"{np.mean(APs):.3f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation Metrics:\n",
        "\n",
        "- Precision@K: Of the top K results, what fraction are relevant?\n",
        "\n",
        "$$\\mathrm{P@K} = \\frac{\\lvert\\text{Relevant in top K}\\rvert}{K}$$\n",
        "\n",
        "- Recall@K: Of all relevant items, how many did we retrieve in top K?\n",
        "\n",
        "$$\\mathrm{R@K} = \\frac{\\lvert\\text{Relevant in top K}\\rvert}{\\lvert\\text{Total relevant}\\rvert}$$\n",
        "\n",
        "- Reciprocal Rank for a query (0 if none):\n",
        "\n",
        "$$\\mathrm{RR} = \\frac{1}{\\text{rank of first relevant}}$$\n",
        "\n",
        "- Mean Reciprocal Rank over queries Q:\n",
        "\n",
        "$$\\mathrm{MRR} = \\frac{1}{\\lvert Q \\rvert} \\sum_{q \\in Q} \\mathrm{RR}_q$$\n",
        "\n",
        "- Discounted Cumulative Gain@K (graded relevance r_i):\n",
        "\n",
        "$$\\mathrm{DCG@K} = \\sum_{i=1}^{K} \\frac{2^{r_i}-1}{\\log_2(i+1)}$$\n",
        "\n",
        "- Normalized DCG@K:\n",
        "\n",
        "$$\\mathrm{NDCG@K} = \\frac{\\mathrm{DCG@K}}{\\mathrm{IDCG@K}}$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "iPhone: P@1=1.000, R@1=1.000\n",
            "iPhone: P@2=1.000, R@2=1.000\n",
            "iPhone: P@3=1.000, R@3=1.000\n"
          ]
        }
      ],
      "source": [
        "# Inline metric helpers (binary relevance)\n",
        "\n",
        "def precision_at_k(relevant_items, retrieved_items, k: int) -> float:\n",
        "    if k == 0 or not retrieved_items:\n",
        "        return 0.0\n",
        "    top_k = set(retrieved_items[:k])\n",
        "    rel = set(relevant_items)\n",
        "    return len(top_k & rel) / min(k, len(retrieved_items))\n",
        "\n",
        "\n",
        "def recall_at_k(relevant_items, retrieved_items, k: int) -> float:\n",
        "    rel = set(relevant_items)\n",
        "    if not rel:\n",
        "        return 0.0\n",
        "    top_k = set(retrieved_items[:k])\n",
        "    return len(top_k & rel) / len(rel)\n",
        "\n",
        "query_relevance = {\n",
        "    'iPhone': {'P001'},\n",
        "    'laptop': {'P005','P006'},\n",
        "    'shoes': {'P003','P004'},\n",
        "}\n",
        "\n",
        "q = 'iPhone'\n",
        "ret = [r['product']['id'] for r in improved_search(df_products, q)]\n",
        "rel = query_relevance[q]\n",
        "\n",
        "for k in [1,2,3]:\n",
        "    p = precision_at_k(rel, ret, k)\n",
        "    r = recall_at_k(rel, ret, k)\n",
        "    print(f\"{q}: P@{k}={p:.3f}, R@{k}={r:.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MRR over ['iPhone', 'laptop', 'shoes']: 1.000\n"
          ]
        }
      ],
      "source": [
        "# MRR across multiple queries (binary relevance)\n",
        "def reciprocal_rank_for_query(relevant_ids, retrieved_ids):\n",
        "    relevant = set(relevant_ids)\n",
        "    for idx, pid in enumerate(retrieved_ids, start=1):\n",
        "        if pid in relevant:\n",
        "            return 1.0/idx\n",
        "    return 0.0\n",
        "\n",
        "queries = ['iPhone','laptop','shoes']\n",
        "\n",
        "query_relevance = {\n",
        "    'iPhone': {'P001'},\n",
        "    'laptop': {'P005','P006'},\n",
        "    'shoes': {'P003','P004'},\n",
        "}\n",
        "\n",
        "rrs = []\n",
        "for q in queries:\n",
        "    ret = [r['product']['id'] for r in improved_search(df_products, q)]\n",
        "    rel = query_relevance[q]\n",
        "    rrs.append(reciprocal_rank_for_query(rel, ret))\n",
        "\n",
        "mrr = float(np.mean(rrs)) if rrs else 0.0\n",
        "print(f\"MRR over {queries}: {mrr:.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "DCG@1=3.000, NDCG@1=1.000\n",
            "DCG@3=3.500, NDCG@3=0.649\n",
            "DCG@5=4.661, NDCG@5=0.864\n"
          ]
        }
      ],
      "source": [
        "# Graded relevance NDCG demo (keeps intuition clear)\n",
        "\n",
        "def dcg_at_k(relevance_scores, k):\n",
        "    dcg = 0.0\n",
        "    for i in range(min(k, len(relevance_scores))):\n",
        "        dcg += (2**relevance_scores[i] - 1) / np.log2(i + 2)\n",
        "    return dcg\n",
        "\n",
        "def ndcg_at_k(relevance_scores, k):\n",
        "    dcg = dcg_at_k(relevance_scores, k)\n",
        "    ideal = sorted(relevance_scores, reverse=True)\n",
        "    idcg = dcg_at_k(ideal, k)\n",
        "    return dcg / idcg if idcg > 0 else 0.0\n",
        "\n",
        "scores = [2,0,1,0,2]\n",
        "for k in [1,3,5]:\n",
        "    print(f\"DCG@{k}={dcg_at_k(scores,k):.3f}, NDCG@{k}={ndcg_at_k(scores,k):.3f}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Organizing Code with a Class\n",
        "Benefits: encapsulation, reusability, maintainability. We'll expose a clean `search()` and reuse earlier tokenization and scoring ideas.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'query': 'iPhone', 'retrieved': ['P001'], 'relevant': ['P001'], 'precision@3': 1.0, 'recall@3': 1.0}\n",
            "{'query': 'laptop', 'retrieved': ['P005', 'P006'], 'relevant': ['P006', 'P005'], 'precision@3': 1.0, 'recall@3': 1.0}\n",
            "{'query': 'shoes', 'retrieved': ['P003', 'P004'], 'relevant': ['P003', 'P004'], 'precision@3': 1.0, 'recall@3': 1.0}\n"
          ]
        }
      ],
      "source": [
        "class SimpleSearchSystem:\n",
        "    def __init__(self, products_df: pd.DataFrame):\n",
        "        self.products_df = products_df\n",
        "        self.query_relevance = {\n",
        "            'iPhone': {'P001'},\n",
        "            'laptop': {'P005','P006'},\n",
        "            'shoes': {'P003','P004'},\n",
        "        }\n",
        "    def tokenize_text(self, text: str):\n",
        "        punctuation_chars = \".,!?;:()[]{}'\\\"-\"\n",
        "        text = str(text).lower()\n",
        "        for ch in punctuation_chars:\n",
        "            text = text.replace(ch, ' ')\n",
        "        return [w for w in text.split() if w]\n",
        "    def search(self, query: str, top_k: int = 5):\n",
        "        qtokens = self.tokenize_text(query)\n",
        "        results = []\n",
        "        for _, row in self.products_df.iterrows():\n",
        "            searchable = self.tokenize_text(f\"{row['title']} {row['description']}\")\n",
        "            matches = sum(1 for t in qtokens if t in searchable)\n",
        "            if matches > 0:\n",
        "                score = matches + (1 if any(t in str(row['title']).lower() for t in qtokens) else 0)\n",
        "                results.append({'product': row.to_dict(), 'score': score, 'matches': matches})\n",
        "        results.sort(key=lambda x: x['score'], reverse=True)\n",
        "        return results[:top_k]\n",
        "    def evaluate_query(self, query: str, k: int = 3):\n",
        "        results = self.search(query, top_k=k)\n",
        "        retrieved = [r['product']['id'] for r in results]\n",
        "        relevant = self.query_relevance.get(query, set())\n",
        "        p = precision_at_k(relevant, retrieved, k)\n",
        "        r = recall_at_k(relevant, retrieved, k)\n",
        "        return {'query': query, 'retrieved': retrieved, 'relevant': list(relevant), f'precision@{k}': p, f'recall@{k}': r}\n",
        "\n",
        "s = SimpleSearchSystem(df_products)\n",
        "for q in ['iPhone','laptop','shoes']:\n",
        "    print(s.evaluate_query(q, k=3))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Search vs Recommendations\n",
        "\n",
        "| Aspect | Search | Recommendations |\n",
        "|---|---|---|\n",
        "| Trigger | User types a query | System suggests proactively |\n",
        "| Intent | Specific, known | Exploratory, discovery |\n",
        "| Query | Required | None required |\n",
        "| Context | Current session | User history, behavior |\n",
        "| Goal | Find exact/close match | Discover related/new items |\n",
        "| Control | User-driven | System-driven |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interview Prep and Key Takeaways\n",
        "\n",
        "- Basic: how search works; search vs recommendations; measuring quality\n",
        "- Technical: typos/normalization; inverted index; ranking signals\n",
        "- Scale: billions of docs; latency budgets; freshness; caching\n",
        "- Business: conversion; A/B tests; internationalization; zero-results handling\n",
        "\n",
        "Key points:\n",
        "- Relevance and latency drive conversion in retail search\n",
        "- Evaluate with P@K, R@K, MRR, NDCG; connect to business metrics\n",
        "- Clean interfaces enable iteration and experiments\n",
        "\n",
        "Next steps: dive into TF-IDF/BM25, query understanding, and learning-to-rank.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Interview Study Guide: What to Know Cold\n",
        "\n",
        "- Core IR concepts\n",
        "  - Inverted index, postings lists, document frequency (DF), term frequency (TF)\n",
        "  - Text processing: tokenization, normalization, stemming vs lemmatization, stopwords, phrase queries\n",
        "  - Query-likelihood vs vector-space models; BM25 intuition vs TF–IDF\n",
        "- Ranking signals and scoring\n",
        "  - Matching features: exact term match, field boosts (title vs description), BM25/TF–IDF\n",
        "  - Business boosts: popularity, freshness, availability, margin, personalization hooks\n",
        "  - Re-ranking: rule-based boosts, learning-to-rank (pointwise/pairwise/listwise)\n",
        "- Query understanding\n",
        "  - Spell correction (edit distance), synonyms, query expansion, entity/attribute detection\n",
        "  - Facets and filters; handling numeric ranges (price), units (inch vs in), categories\n",
        "- Evaluation (offline)\n",
        "  - Precision@K, Recall@K, MAP, MRR, NDCG; graded vs binary relevance\n",
        "  - Labeling strategies: heuristics, click-derived labels, human annotation\n",
        "- Evaluation (online)\n",
        "  - A/B testing basics: metrics (CTR, Conversion, Revenue/Search, Add-to-cart), guardrails (latency, zero-results rate)\n",
        "  - Power, MDE, significance, sample size; experiment length and bucketing\n",
        "- E-commerce specifics\n",
        "  - Query intent (navigational vs informational vs transactional), SKU vs product grouping\n",
        "  - Typos, variants (\"tee\" vs \"t-shirt\"), attribute-aware search (color/size/brand), internationalization\n",
        "  - Merchandising rules, inventory/freshness, cold start, seasonality\n",
        "- System design at a glance\n",
        "  - Indexing pipeline, shards/replicas, caching (Redis), search API (FastAPI), logging/monitoring (latency, p95/p99)\n",
        "  - Backfills, reindexing strategies, blue/green deploys, safety switches\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
